{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI4D News Classification",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00O7oqE3l53K"
      },
      "source": [
        "## import libraries to be used "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5o2LoNoR7Aj"
      },
      "source": [
        "#import libraries\r\n",
        "import pandas as pd \r\n",
        "import numpy as np\r\n",
        "import re \r\n",
        "\r\n",
        "import matplotlib.pyplot as plt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYiBR0nKSnpL"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.ensemble import BaggingClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\r\n",
        "from sklearn.metrics import log_loss\r\n",
        "\r\n",
        "from string import punctuation\r\n",
        "import warnings \r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "# seeding\r\n",
        "np.random.seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcnAiU8AmBfe"
      },
      "source": [
        "## load the datasets ie train,test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmqc1_A-Ss7A"
      },
      "source": [
        "#load data \r\n",
        "train = pd.read_csv('train.csv', error_bad_lines=False)\r\n",
        "test = pd.read_csv('test.csv')\r\n",
        "sub= pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75yA1XtLmG94"
      },
      "source": [
        "### shape of the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K3GqiVzUOdw"
      },
      "source": [
        "print(train.shape)\r\n",
        "print(test.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whcc4YswmKu0"
      },
      "source": [
        "## Data Preprocessing\r\n",
        "### mapping categorical values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7mUlweBUSAD"
      },
      "source": [
        "#data preprocessing \r\n",
        "# a mapping dictionary that maps the category values from 0 to 5\r\n",
        "category_mapping = {\r\n",
        "\"kitaifa\": 0,\r\n",
        "\"michezo\": 1,\r\n",
        "\"burudani\": 2,\r\n",
        "\"uchumi\": 3,\r\n",
        "\"kimataifa\": 4,\r\n",
        "\"afya\": 5\r\n",
        "}\r\n",
        "\r\n",
        "train[\"category\"] = train.category.map(category_mapping)\r\n",
        "\r\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1FL2IvAMHjB"
      },
      "source": [
        "train.category.value_counts().plot.barh()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi5dzm2vmR-c"
      },
      "source": [
        "### list of stopwords in swahili"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WGYTVOpUeGE"
      },
      "source": [
        "sw_stopwords=[\"akasema\",\"alikuwa\",\"alisema\",\"baada\",\"basi\",\"bila\",\"cha\",\"chini\",\"hadi\",\r\n",
        "              \"hapo\",\"hata\",\"hivyo\",\"hiyo\",\"huku\",\"huo\",\"ili\",\"ilikuwa\",\"juu\",\"kama\",\"karibu\",\r\n",
        "              \"katika\",\"kila\",\"kima\",\"kisha\",\"kubwa\",\"kutoka\",\"kuwa\",\"kwa\",\"kwamba\",\"kwenda\",\"kwenye\",\"la\",\"lakini\",\r\n",
        "              \"mara\",\"mdogo\",\"mimi\",\"mkubwa\",\"mmoja\",\"moja\",\"muda\",\"mwenye\",\"na\",\"naye\",\"ndani\",\"ng\",\"ni\",\"nini\",\r\n",
        "              \"nonkungu\",\"pamoja\",\"pia\",\"sana\",\"sasa\",\"sauti\",\"tafadhali\",\"tena\",\"tu\",\"vile\",\"wa\",\r\n",
        "              \"wakati\",\"wake\",\"walikuwa\",\"wao\",\"watu\",\"wengine\",\"wote\",\"ya\",\"yake\",\"yangu\",\"yao\",\"yeye\",\"yule\",\"za\",\r\n",
        "              \"zaidi\",\"zake\",\"na\",\"ya\",\"wa\",\"kwa\",\"ni\",\"za\",\"katika\",\"la\",\"kuwa\",\"kama\",\"kwamba\",\"cha\",\"hiyo\",\"lakini\",\"yake\",\"hata\",\"wakati\",\r\n",
        "              \"hivyo\",\"sasa\",\"wake\",\"au\",\"watu\",\"hii\",\"zaidi\",\"vya\",\"huo\",\"tu\",\"kwenye\",\"si\",\"pia\",\"ili\",\"moja\",\"kila\",\"baada\",\"ambao\",\"ambayo\",\"yao\",\"wao\",\"kuna\",\r\n",
        "              \"hilo\",\"kutoka\",\"kubwa\",\"pamoja\",\"bila\",\"huu\",\"hayo\",\"sana\",\"ndani\",\"mkuu\",\"hizo\",\"kufanya\",\"wengi\",\"hadi\",\"mmoja\",\"hili\",\"juu\",\"kwanza\",\"wetu\",\"kuhusu\",\r\n",
        "              \"baadhi\",\"wote\",\"yetu\",\"hivi\",\"kweli\",\"mara\",\"wengine\",\"nini\",\"ndiyo\",\"zao\",\"kati\",\"hao\",\"hapa\",\"kutokana\",\"muda\",\"habari\",\"ambaye\",\"wenye\",\"nyingine\",\"hakuna\",\r\n",
        "              \"tena\",\"hatua\",\"bado\",\"nafasi\",\"basi\",\"kabisa\",\"hicho\",\"nje\",\"huyo\",\"vile\",\"yote\",\"mkubwa\",\"alikuwa\",\"zote\",\"leo\",\"haya\",\"huko\",\"kutoa\",\"mwa\",\"kiasi\",\"hasa\",\"nyingi\",\"kabla\",\"wale\",\"chini\",\"gani\",\"hapo\",\"lazima\",\"mwingine\",\"bali\",\"huku\",\"zake\",\"ilikuwa\",\r\n",
        "              \"tofauti\",\"kupata\",\"mbalimbali\",\"pale\",\"kusema\",\"badala\",\"wazi\",\"yeye\",\"alisema\",\"hawa\",\r\n",
        "              \"ndio\",\"hizi\",\"tayari\",\"wala\",\"muhimu\",\"ile\",\"mpya\",\"ambazo\",\"dhidi\",\"kwenda\",\"sisi\",\"kwani\",\r\n",
        "              \"jinsi\",\"binafsi\",\"kutumia\",\"mbili\",\"mbali\",\"kuu\",\"mengine\",\"mbele\",\"namna\",\"mengi\",\"upande\",\"na\",\"lakini\",\"ingawa\"\r\n",
        "              \"ingawaje\",\"kwa\",\"sababu\",\"hadi\",\"hata\",\"kama\",\"ambapo\",\"ambamo\",\"ambako\",\"ambacho\",\"ambao\",\"ambaye\",\"ilhali\",\"ya\",\"yake\",\"yao\",\"yangu\",\"yetu\",\"yenu\",\"vya\",\"vyao\",\"vyake\",\"vyangu\",\r\n",
        "\"vyenu\",\"vyetu\",\"yako\",\"yao\",\"hizo\",\"yenu\",\"mimi\",\"sisi\",\"wewe\",\"nyinyi\",\"yeye\",\"wao\",\"nao\",\"nasi\",\"nanyi\",\"ni\",\"alikuwa\",\"atakuwa\",\"hii\",\"hizi\",\"zile\",\r\n",
        "\"ile\",\"hivi\",\"vile\",\"za\",\"zake\",\"zao\",\"zenu\",\"kwenye\",\"katika\",\"kwa\",\"kwao\",\"kwenu\",\"kwetu\",\"dhidi\",\"kati\",\"miongoni\",\"katikati\",\"wakati\",\"kabla\",\"baada\",\r\n",
        "\"baadaye\",\"nje\",\"tena\",\"mbali\",\"halafu\",\"hapa\",\"pale\",\"mara\",\"mara\",\"yoyote\",\"wowote\",\"chochote\",\"vyovyote\",\"yeyote\",\"lolote\",\"mwenye\",\"mwenyewe\",\"lenyewe\",\r\n",
        "\"lenye\",\"wote\",\"lote\",\"vyote\",\"nyote\",\"kila\",\"zaidi\",\"hapana\",\"ndiyo\",\"au\",\"ama\",\"ama\",\"sio\",\"siye\",\"tu\",\"budi\",\"nyingi\",\"nyingine\",\"wengine\",\"mwingine\",\r\n",
        "\"zingine\",\"lingine\",\"kingine\",\"chote\",\"sasa\",\"basi\",\"bila\",\"cha\",\"chini\",\"hapo\",\"pale\",\"huku\",\"kule\",\"humu\",\"hivyo\",\"hivyohivyo\",\"vivyo\",\"palepale\",\"fauka\",\r\n",
        "\"hiyo\",\"hiyohiyo\",\"zile\",\"zilezile\",\"hao\",\"haohao\",\"huku\",\"hukuhuku\",\"humuhumu\",\"huko\",\"hukohuko\",\"huo\",\"huohuo\",\"hili\",\"hilihili\",\"ilikuwa\",\"juu\",\"karibu\",\r\n",
        "\"kila\",\"kima\",\"kisha\",\"kutoka\",\"kwenda\",\"kubwa\",\"ndogo\",\"kwamba\",\"kuwa\",\"la\",\"lao\",\"lo\",\"mara\",\"na\",\r\n",
        "\"mdogo\",\"mkubwa\",\"ngâ€™o\",\"pia\",\"aidha\",\"vile\",\"vilevile\",\"kadhalika\",\"halikadhalika\",\"ni\",\"sana\",\"pamoja\",\"pamoja\",\"tafadhali\",\"tena\",\r\n",
        "\"wa\",\"wake\",\"wao\",\r\n",
        "\"ya\",\"yule\",\"wale\",\"zangu\",\"nje\",\"afanaleki\",\"salale\",\"oyee\",\"yupi\",\"ipi\",\"lipi\",\"ngapi\",\"yetu\",\"si\",\"angali\",\"wangali\",\"loo\",\"la\",\"ohoo\",\r\n",
        "\"barabara\",\"oyee\",\r\n",
        "\"ewaa\",\"walahi\",\"masalale\",\"duu\",\"toba\",\"mh\",\"kumbe\",\"ala\",\"ebo\",\"haraka\",\"pole\",\"polepole\",\"harakaharaka\",\"hiyo\",\"hivyo\",\"vyovyote\",\r\n",
        "\"atakuwa\",\"itakuwa\",\"mtakuwa\",\r\n",
        "\"tutakuwa\",\"labda\",\"yumkini\",\"haiyumkini\",\"yapata\",\"takribani\",\"hususani\",\"yawezekana\",\"nani\",\"juu\"\"chini\",\r\n",
        "\"ndani\",\"baadhi\",\"kuliko\",\"vile\",\"mwa\",\"kwa\",\"hasha\",\"hivyo\",\"moja\",\"kisha\",\r\n",
        "\"pili\",\"kwanza\",\"ili\",\"je\",\"jinsi\",\"ila\",\"ila\",\"nini\",\"hasa\",\"huu\",\"zako\",\"mimi\",\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVLOtjbWmasw"
      },
      "source": [
        "## import nltk for Natural Language Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIHIsBQTUiEM"
      },
      "source": [
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb7ZkrBzUlAW"
      },
      "source": [
        "trainc= train.copy()\r\n",
        "testc=test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJEHl3ofmhga"
      },
      "source": [
        "### clean our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cH1KACXUnhJ"
      },
      "source": [
        "import string\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\r\n",
        "    and remove words containing numbers.'''\r\n",
        "    text = text.lower()\r\n",
        "    text = re.sub('\\[.*?\\]', '', text)\r\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\r\n",
        "    text = re.sub('<.*?>+', '', text)\r\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\r\n",
        "    text = re.sub('\\n', '', text)\r\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\r\n",
        "    return text\r\n",
        "\r\n",
        "\r\n",
        "# Applying the cleaning function to both test and training datasets\r\n",
        "train['content'] = train['content'].apply(lambda x: clean_text(x))\r\n",
        "test['content'] = test['content'].apply(lambda x: clean_text(x))\r\n",
        "\r\n",
        "# Let's take a look at the updated text\r\n",
        "train['content'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbNTIQDdmmjx"
      },
      "source": [
        "### Tokenize our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiKI7KYNUqCO"
      },
      "source": [
        "# Tokenizing the training and the test set\r\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\r\n",
        "train['content'] = train['content'].apply(lambda x: tokenizer.tokenize(x))\r\n",
        "test['content'] = test['content'].apply(lambda x: tokenizer.tokenize(x))\r\n",
        "train['content'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L65obhkemweM"
      },
      "source": [
        "### Remove Stopwords in Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UIRNUfQUtMz"
      },
      "source": [
        "def remove_stopwords(text):\r\n",
        "    \"\"\"\r\n",
        "    Removing stopwords belonging to swahili language\r\n",
        "    \r\n",
        "    \"\"\"\r\n",
        "    words = [w for w in text if w not in sw_stopwords]\r\n",
        "    return words\r\n",
        "\r\n",
        "train['content'] = train['content'].apply(lambda x : remove_stopwords(x))\r\n",
        "test['content'] = test['content'].apply(lambda x : remove_stopwords(x))\r\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Graj-TgOUwI1"
      },
      "source": [
        "# After preprocessing, the text format\r\n",
        "def combine_text(list_of_text):\r\n",
        "    '''Takes a list of text and combines them into one large chunk of text.'''\r\n",
        "    combined_text = ' '.join(list_of_text)\r\n",
        "    return combined_text\r\n",
        "train['content']=train['content'].apply(lambda x: combine_text(x))\r\n",
        "test['content'] = test['content'].apply(lambda x : combine_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2YsiCTEm16y"
      },
      "source": [
        "### TFIDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKqYv1fPUyWJ"
      },
      "source": [
        "tfidf = TfidfVectorizer(min_df=15, max_df=0.5, ngram_range=(1, 2),norm='l2',sublinear_tf=True)\r\n",
        "train_vectors = tfidf.fit_transform(train['content'])\r\n",
        "test_vectors = tfidf.transform(test[\"content\"])\r\n",
        "#10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJC7H5ORm5NE"
      },
      "source": [
        "## Train our model and evaluate the log loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu9dxc30U2jK"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from xgboost import XGBClassifier\r\n",
        "\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\r\n",
        "from sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY-fxqAznBAF"
      },
      "source": [
        "### split our data into train and test set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2qgm3TjVAEH"
      },
      "source": [
        "#split our data into train and test\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "#split features and target from train data \r\n",
        "X = train_vectors\r\n",
        "y = train.category.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ7YXX3kPrqx"
      },
      "source": [
        "#from sklearn.preprocessing import MinMaxScaler\r\n",
        "\r\n",
        "#scaler = MinMaxScaler()\r\n",
        "#X_norm = scaler.fit_transform(X.toarray())\r\n",
        "#X_test_norm = scaler.transform(test_vectors.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxuihELrVDom"
      },
      "source": [
        "# split data into train and validate\r\n",
        "\r\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\r\n",
        "    X,\r\n",
        "    y,\r\n",
        "    test_size=0.20,\r\n",
        "    random_state=42,\r\n",
        "    shuffle=True,\r\n",
        "    stratify=y,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jQ8UFVrVF_B"
      },
      "source": [
        "# Create a classifier\r\n",
        "news_classifier = MultinomialNB()\r\n",
        "\r\n",
        "# train the news_classifier \r\n",
        "news_classifier.fit(X_train,y_train)\r\n",
        "# test model performance on valid data \r\n",
        "y_probas = news_classifier.predict_proba(X_valid)\r\n",
        "# evalute model performance by using log_loss in the validation data\r\n",
        "log_loss(y_valid, y_probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpTCtbeNVNGm"
      },
      "source": [
        "# create prediction from the test data\r\n",
        "test_probas = news_classifier.predict_proba(test_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRyNcUZ0VuE6"
      },
      "source": [
        "rfr=RandomForestClassifier(n_estimators=150, max_depth=6, random_state=0)\r\n",
        "rfr.fit(X_train,y_train)\r\n",
        "rfr_probas= rfr.predict_proba(X_valid)\r\n",
        "log_loss(y_valid,rfr_probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV_wssLmWaEn"
      },
      "source": [
        "xgb=XGBClassifier()\r\n",
        "xgb.fit(X_train,y_train)\r\n",
        "xgb_probas= xgb.predict_proba(X_valid)\r\n",
        "log_loss(y_valid,xgb_probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RGL0R8iFiwt"
      },
      "source": [
        "#logistic regression\r\n",
        "logr=LogisticRegression(max_iter=150,C=7, random_state=23)\r\n",
        "logr.fit(X_train,y_train)\r\n",
        "logr_probas= logr.predict_proba(X_valid)\r\n",
        "log_loss(y_valid,logr_probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBEjFy1gYMzk"
      },
      "source": [
        "# create prediction from the test data\r\n",
        "logr_probass =logr.predict_proba(test_vectors)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRzarDqGnke6"
      },
      "source": [
        "### create a submission file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buoMSsHJVRV6"
      },
      "source": [
        "# create submission file \r\n",
        "submission_cols = ['kitaifa', 'michezo', 'burudani','uchumi', 'kimataifa', 'afya'] \r\n",
        "submission_df = pd.DataFrame(logr_probass, columns = submission_cols)\r\n",
        "submission_df['test_id'] = sub['test_id']   # add  test_id \r\n",
        "\r\n",
        "#rearange columns \r\n",
        "submission_df = submission_df[['test_id','kitaifa', 'michezo', 'burudani','uchumi', 'kimataifa', 'afya']]\r\n",
        "\r\n",
        "# save submission file \r\n",
        "submission_df.to_csv(\"first_submission.csv\",index=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRU1T2DWVT5S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}